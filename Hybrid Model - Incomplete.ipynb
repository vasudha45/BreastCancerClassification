{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWnT3gwZCsTo",
        "outputId": "1cab9a39-885e-4335-9ef4-662fb484b190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Set seed for reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "# Paths\n",
        "original_dataset_dir = '/content/drive/MyDrive/Dataset'\n",
        "base_dir = 'dataset'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "val_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "# Create new directories\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(val_dir, exist_ok=True)\n",
        "\n",
        "# Categories\n",
        "categories = ['benign', 'malignant']\n",
        "\n",
        "for category in categories:\n",
        "    os.makedirs(os.path.join(train_dir, category), exist_ok=True)\n",
        "    os.makedirs(os.path.join(val_dir, category), exist_ok=True)\n",
        "\n",
        "    category_dir = os.path.join(original_dataset_dir, category)\n",
        "    images = os.listdir(category_dir)\n",
        "    random.shuffle(images)\n",
        "\n",
        "    # Split the images into 80% training and 20% validation\n",
        "    train_split = int(0.8 * len(images))\n",
        "    train_images = images[:train_split]\n",
        "    val_images = images[train_split:]\n",
        "\n",
        "    for image in train_images:\n",
        "        src = os.path.join(category_dir, image)\n",
        "        dst = os.path.join(train_dir, category, image)\n",
        "        shutil.copyfile(src, dst)\n",
        "\n",
        "    for image in val_images:\n",
        "        src = os.path.join(category_dir, image)\n",
        "        dst = os.path.join(val_dir, category, image)\n",
        "        shutil.copyfile(src, dst)\n",
        "\n",
        "print(\"Dataset restructured successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oL4kHvmpCunI",
        "outputId": "7f975638-13ba-4046-94b2-c9dcd9318c5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset restructured successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hybrid Model"
      ],
      "metadata": {
        "id": "gdJdO3LcDEQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vit-keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzpWA8n4EQOQ",
        "outputId": "7a4ed99c-542a-487f-92c8-cd11d08b540f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vit-keras\n",
            "  Downloading vit_keras-0.1.2-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from vit-keras) (1.11.4)\n",
            "Collecting validators (from vit-keras)\n",
            "  Downloading validators-0.31.0-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->vit-keras) (1.25.2)\n",
            "Installing collected packages: validators, vit-keras\n",
            "Successfully installed validators-0.31.0 vit-keras-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons\n",
        "!pip install vit-keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2mj43KNF3A1",
        "outputId": "11b16557-3573-417f-da70-08ee083600fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (24.1)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.23.0 typeguard-2.13.3\n",
            "Requirement already satisfied: vit-keras in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from vit-keras) (1.11.4)\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.10/dist-packages (from vit-keras) (0.31.0)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->vit-keras) (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_addons as tfa\n",
        "from vit_keras import vit\n",
        "\n",
        "print(\"tensorflow-addons and vit-keras installed successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QH-jr8CmGAnM",
        "outputId": "3cbd0c92-23da-49e6-af42-903501d81bea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensorflow-addons and vit-keras installed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input, Conv2D, MaxPooling2D, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from vit_keras import vit\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "# Data augmentation and normalization for training\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Validation data should only be rescaled\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Generators\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/content/dataset/train',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    color_mode='rgb',  # Convert grayscale images to RGB\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "    '/content/dataset/validation',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    color_mode='rgb',  # Convert grayscale images to RGB\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# CNN Model for local feature extraction\n",
        "def create_cnn_model(input_shape):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Conv2D(32, (3, 3), activation='relu')(inputs)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Conv2D(128, (3, 3), activation='relu')(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Flatten()(x)\n",
        "    return Model(inputs, x)\n",
        "\n",
        "cnn_model = create_cnn_model((224, 224, 3))\n",
        "\n",
        "# Vision Transformer (ViT) model\n",
        "vit_model = vit.vit_b16(\n",
        "    image_size=224,\n",
        "    pretrained=True,\n",
        "    include_top=False,\n",
        "    pretrained_top=False\n",
        ")\n",
        "\n",
        "# Combine CNN and ViT\n",
        "combined_input = cnn_model.input\n",
        "combined_output = cnn_model.output\n",
        "vit_output = vit_model(combined_input)\n",
        "\n",
        "# Concatenate the outputs of CNN and ViT\n",
        "x = tf.keras.layers.Concatenate()([combined_output, vit_output])\n",
        "x = Dropout(0.5)(x)  # Dropout for regularization\n",
        "x = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Create the hybrid model\n",
        "model = Model(inputs=combined_input, outputs=x)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "checkpoint_path = \"/content/drive/MyDrive/Project/best_hybrid_model.h5\"\n",
        "early_stopping_cb = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "reduce_lr_cb = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)\n",
        "checkpoint_cb = ModelCheckpoint(filepath=checkpoint_path, save_best_only=True, monitor='val_accuracy', mode='max', verbose=1, save_format='h5')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    epochs=20,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
        "    callbacks=[checkpoint_cb, early_stopping_cb, reduce_lr_cb]\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = model.evaluate(validation_generator)\n",
        "print(f\"Validation accuracy: {accuracy}\")\n",
        "\n",
        "# Load the best model\n",
        "best_model = tf.keras.models.load_model(checkpoint_path)\n",
        "\n",
        "# Save the best model explicitly\n",
        "best_model.save('/content/drive/MyDrive/Project/best_hybrid_model_final.h5')\n",
        "\n",
        "# Predict on the validation set\n",
        "validation_generator.reset()\n",
        "y_pred = best_model.predict(validation_generator)\n",
        "y_pred = np.round(y_pred).astype(int).flatten()\n",
        "y_true = validation_generator.classes\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "cm_plot_labels = ['benign', 'malignant']\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=cm_plot_labels, yticklabels=cm_plot_labels)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Classification report\n",
        "print(classification_report(y_true, y_pred, target_names=cm_plot_labels))\n",
        "\n",
        "# Plot training history\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(len(acc))\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRfMczXvC_9n",
        "outputId": "fa5bdb7a-0656-4738-c238-3eddf5e58187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8000 images belonging to 2 classes.\n",
            "Found 1999 images belonging to 2 classes.\n",
            "Downloading data from https://github.com/faustomorales/vit-keras/releases/download/dl/ViT-B_16_imagenet21k+imagenet2012.npz\n",
            "347502902/347502902 [==============================] - 1s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/vit_keras/utils.py:81: UserWarning: Resizing position embeddings from 24, 24 to 14, 14\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "250/250 [==============================] - ETA: 0s - loss: 0.5803 - accuracy: 0.6735\n",
            "Epoch 1: val_accuracy improved from -inf to 0.72278, saving model to /content/drive/MyDrive/Project/best_hybrid_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r250/250 [==============================] - 430s 2s/step - loss: 0.5803 - accuracy: 0.6735 - val_loss: 0.5145 - val_accuracy: 0.7228 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "250/250 [==============================] - ETA: 0s - loss: 0.5139 - accuracy: 0.7199\n",
            "Epoch 2: val_accuracy improved from 0.72278 to 0.74748, saving model to /content/drive/MyDrive/Project/best_hybrid_model.h5\n",
            "250/250 [==============================] - 400s 2s/step - loss: 0.5139 - accuracy: 0.7199 - val_loss: 0.5394 - val_accuracy: 0.7475 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "250/250 [==============================] - ETA: 0s - loss: 0.5021 - accuracy: 0.7360\n",
            "Epoch 3: val_accuracy did not improve from 0.74748\n",
            "250/250 [==============================] - 360s 1s/step - loss: 0.5021 - accuracy: 0.7360 - val_loss: 0.4821 - val_accuracy: 0.7445 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "250/250 [==============================] - ETA: 0s - loss: 0.4852 - accuracy: 0.7398\n",
            "Epoch 4: val_accuracy improved from 0.74748 to 0.76109, saving model to /content/drive/MyDrive/Project/best_hybrid_model.h5\n",
            "250/250 [==============================] - 404s 2s/step - loss: 0.4852 - accuracy: 0.7398 - val_loss: 0.4488 - val_accuracy: 0.7611 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "250/250 [==============================] - ETA: 0s - loss: 0.4727 - accuracy: 0.7521\n",
            "Epoch 5: val_accuracy improved from 0.76109 to 0.76865, saving model to /content/drive/MyDrive/Project/best_hybrid_model.h5\n",
            "250/250 [==============================] - 397s 2s/step - loss: 0.4727 - accuracy: 0.7521 - val_loss: 0.4361 - val_accuracy: 0.7686 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "250/250 [==============================] - ETA: 0s - loss: 0.4539 - accuracy: 0.7707\n",
            "Epoch 6: val_accuracy did not improve from 0.76865\n",
            "250/250 [==============================] - 358s 1s/step - loss: 0.4539 - accuracy: 0.7707 - val_loss: 0.4653 - val_accuracy: 0.7611 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "250/250 [==============================] - ETA: 0s - loss: 0.4459 - accuracy: 0.7696\n",
            "Epoch 7: val_accuracy improved from 0.76865 to 0.80393, saving model to /content/drive/MyDrive/Project/best_hybrid_model.h5\n",
            "250/250 [==============================] - 407s 2s/step - loss: 0.4459 - accuracy: 0.7696 - val_loss: 0.3947 - val_accuracy: 0.8039 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "250/250 [==============================] - ETA: 0s - loss: 0.4308 - accuracy: 0.7856\n",
            "Epoch 8: val_accuracy improved from 0.80393 to 0.81250, saving model to /content/drive/MyDrive/Project/best_hybrid_model.h5\n",
            "250/250 [==============================] - 392s 2s/step - loss: 0.4308 - accuracy: 0.7856 - val_loss: 0.3830 - val_accuracy: 0.8125 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "133/250 [==============>...............] - ETA: 2:35 - loss: 0.4163 - accuracy: 0.7953"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y-yocZAEEIbK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}